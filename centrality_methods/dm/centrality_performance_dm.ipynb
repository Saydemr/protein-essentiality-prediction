{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "centrality_methods = [\"data/betweenness_centrality_ess.csv\", \"data/closeness_centrality_ess.csv\", \n",
    "          \"data/degree_centrality_ess.csv\", \"data/eigenvector_centrality_ess.csv\", \n",
    "          \"data/information_centrality_ess.csv\"]\n",
    "fill = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Centrality_Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC AUC</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Centrality_Method, Accuracy, F1 Score, ROC AUC, Precision, Recall, Time]\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame(columns= ['Centrality_Method', 'Accuracy', 'F1 Score', 'ROC AUC' 'Precision', 'Recall', 'Time'])\n",
    "results.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>__</th>\n",
       "      <th>Predicted_Positive</th>\n",
       "      <th>Predicted_Negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Actual_Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Actual_Negative</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                __ Predicted_Positive Predicted_Negative\n",
       "0  Actual_Positive                NaN                NaN\n",
       "1  Actual_Negative                NaN                NaN"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagram = pd.DataFrame(columns= ['__', 'Predicted_Positive', 'Predicted_Negative'])\n",
    "col = ['Actual_Positive', 'Actual_Negative']\n",
    "diagram['__'] = col\n",
    "diagram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Metric:  data/betweenness_centrality_ess.csv\n",
      "Accuracy:  0.95237560372908\n",
      "F1 Score:  0.5704214132442602\n",
      "Precision:  0.5704214132442602\n",
      "Recall:  0.5704214132442602\n",
      "ROC AUC: 0.5704214132442601\n",
      "time taken:  0.03621172904968262\n",
      "\n",
      "\n",
      "Metric:  data/closeness_centrality_ess.csv\n",
      "Accuracy:  0.9528248904863529\n",
      "F1 Score:  0.5744740414212011\n",
      "Precision:  0.5744740414212011\n",
      "Recall:  0.5744740414212011\n",
      "ROC AUC: 0.5744740414212011\n",
      "time taken:  0.014510154724121094\n",
      "\n",
      "\n",
      "Metric:  data/degree_centrality_ess.csv\n",
      "Accuracy:  0.9505784566999887\n",
      "F1 Score:  0.5542109005364965\n",
      "Precision:  0.5542109005364965\n",
      "Recall:  0.5542109005364965\n",
      "ROC AUC: 0.5542109005364964\n",
      "time taken:  0.013826370239257812\n",
      "\n",
      "\n",
      "Metric:  data/eigenvector_centrality_ess.csv\n",
      "Accuracy:  0.9483320229136246\n",
      "F1 Score:  0.5339477596517918\n",
      "Precision:  0.5339477596517918\n",
      "Recall:  0.5339477596517918\n",
      "ROC AUC: 0.5339477596517918\n",
      "time taken:  0.012847900390625\n",
      "\n",
      "\n",
      "Metric:  data/information_centrality_ess.csv\n",
      "Accuracy:  0.9508031000786252\n",
      "F1 Score:  0.5562372146249669\n",
      "Precision:  0.5562372146249669\n",
      "Recall:  0.5562372146249669\n",
      "ROC AUC: 0.5562372146249669\n",
      "time taken:  0.012104511260986328\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in centrality_methods:\n",
    "    first = time.time()\n",
    "    centrality_df = pd.read_csv(i)\n",
    "    results.at[i, 'Centrality_Method'] = i\n",
    "    index = 0\n",
    "\n",
    "    actuals = centrality_df['Essentiality'].to_numpy()\n",
    "    preds = np.concatenate((np.ones(254, dtype=int),np.zeros(actuals.shape[0]- 254, dtype=int)))\n",
    "\n",
    "    \n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "    accuracy = accuracy_score(actuals, preds)\n",
    "    precision = precision_score(actuals, preds, average='macro')\n",
    "    recall = recall_score(actuals, preds, average='macro')\n",
    "    roc_auc = roc_auc_score(actuals, preds, average='macro')\n",
    "    f1 = f1_score(actuals, preds, average='macro')\n",
    "\n",
    "\n",
    "    last = time.time()\n",
    "\n",
    "    row = [i, accuracy, f1, roc_auc, precision, recall, last-first]\n",
    "    results.loc[i] = row\n",
    "    results.to_csv('data/results.csv', index=False)\n",
    "\n",
    "    print()\n",
    "    print(\"Metric: \", i)\n",
    "    print('Accuracy: ', accuracy)\n",
    "    print('F1 Score: ', f1)\n",
    "    print('Precision: ', precision)\n",
    "    print('Recall: ', recall)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(\"time taken: \", last-first)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "11938c6bc6919ae2720b4d5011047913343b08a43b18698fd82dedb0d4417594"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
